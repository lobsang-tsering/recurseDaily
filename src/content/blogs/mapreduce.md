---
title: "MapReduce"
pubDate: "Jul 08 2022"
heroImage: "/blog-placeholder-3.jpg"
difficulty: "Kafka"
description: "data structure store"
topics: ["mapreduce", "distributed system"]
---

# MapReduce: Simplified Data Processing on Large Clusters

## ğŸ§  Deep-Dive Notes

### ğŸ“˜ Introduction

- **Goal:** Simplify large-scale data processing across thousands of machines.
- **Problem:** Writing distributed programs is hardâ€”needs handling of parallelization, fault-tolerance, data distribution, and load balancing.
- **Solution:** Provide an abstractionâ€”**MapReduce**â€”inspired by Lisp primitives `map()` and `reduce()`.

### ğŸ§© Programming Model

- **Map Function:**
  ```python
  map(key1, value1) â†’ list(key2, value2)
  ```
- **Reduce Function:**
  ```python
  reduce(key2, list(value2)) â†’ list(value3)
  ```
- **Example (Word Count):**
  - Map emits (word, 1) for each word.
  - Reduce sums up the counts for each word.

### âš™ï¸ Execution Overview

1. **Split Input:** Input data is split into **M** splits.
2. **Start Tasks:**
   - **Map Tasks (M):** Process input splits and generate intermediate pairs.
   - **Reduce Tasks (R):** Group intermediate pairs and apply reduce function.
3. **Data Flow:**
   - Intermediate data written to local disk.
   - Master informs reducers where to read from.
   - Reducers fetch data, sort by key, group values, and run reduce.
4. **Output:** Final output is written to distributed file system (e.g., GFS).

### ğŸ§± Fault Tolerance

- **Task re-execution:** On failure, master reschedules the task on another worker.
- **Data re-processing:** Since the Map/Reduce functions are pure (no side effects), re-execution is safe.
- **Stragglers:** Backup tasks started for slow-running workers to reduce latency.

### ğŸš€ Optimizations

- **Locality-aware scheduling:** Map tasks prefer data-local blocks.
- **Combiner Functions:** Mini-reduce at map-side to reduce data shuffle volume.
- **Partitioning Function:** Determines which reducer gets which key.

### ğŸ“Š Performance

- Google used it to sort **1TB of data in under 900 seconds** on a 1800-machine cluster.
- Highly scalableâ€”used for web indexing, log analysis, machine learning, etc.

### ğŸ“¦ Real-World Applications at Google

- Indexing web pages
- Extracting data from logs
- Generating reports
- Machine Learning preprocessing

### ğŸ† Key Contributions

1. **Simplicity:** Developers only need to define Map and Reduce logic.
2. **Scalability:** Easily scales to petabytes.
3. **Fault Tolerance:** Robust against hardware/software failures.
4. **Reusability:** Adaptable to many problems in distributed computing.

---

## ğŸ§­ Mind Map â€“ MapReduce

```
MapReduce: Simplified Data Processing on Large Clusters
|
â”œâ”€â”€ Concept
|   â”œâ”€â”€ Inspired by map() and reduce()
|   â”œâ”€â”€ Abstracts parallel computation
|
â”œâ”€â”€ Programming Model
|   â”œâ”€â”€ Map(key1, value1) â†’ list(key2, value2)
|   â””â”€â”€ Reduce(key2, list(value2)) â†’ list(value3)
|
â”œâ”€â”€ Execution
|   â”œâ”€â”€ Split input â†’ M map tasks
|   â”œâ”€â”€ Map phase â†’ intermediate (key, value)
|   â”œâ”€â”€ Shuffle & sort
|   â””â”€â”€ R reduce tasks â†’ final output
|
â”œâ”€â”€ Features
|   â”œâ”€â”€ Fault tolerance
|   â”œâ”€â”€ Data locality optimization
|   â”œâ”€â”€ Backup tasks for stragglers
|   â””â”€â”€ Combiner for intermediate aggregation
|
â”œâ”€â”€ Applications
|   â”œâ”€â”€ Web indexing
|   â”œâ”€â”€ Log analysis
|   â”œâ”€â”€ Report generation
|   â””â”€â”€ Machine learning preprocessing
|
â””â”€â”€ Impact
    â”œâ”€â”€ Simplified distributed programming
    â”œâ”€â”€ Inspired Hadoop and Spark
    â””â”€â”€ Scalable & robust design
```
